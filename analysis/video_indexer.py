#!/usr/bin/env python3
# coding: utf-8
"""
video_indexer_simple.py
- æ—¢å­˜ã®media_indexer_lmstudio.pyã®process_imageé–¢æ•°ã‚’ãƒ™ãƒ¼ã‚¹ã«å‹•ç”»è§£æ
- FFmpegã§ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå¾Œã€æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜å‡¦ç†ãƒ•ãƒ­ãƒ¼
"""
from pathlib import Path
import argparse
import json
import sys
import tempfile
import os
import hashlib
import subprocess
import shutil
from typing import Optional, List, Dict
try:
    if hasattr(sys.stdout, "reconfigure"):
        sys.stdout.reconfigure(encoding="utf-8")
    if hasattr(sys.stderr, "reconfigure"):
        sys.stderr.reconfigure(encoding="utf-8")
except Exception:
    pass
import ffmpeg
from tqdm import tqdm

# æ—¢å­˜ã®ç”»åƒè§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from image_indexer import (
    load_env, convert_and_resize_to_jpeg, call_lmstudio_vlm, 
    parse_response_text, upsert_meilisearch, ensure_qdrant_collection, 
    upsert_qdrant_vector, path_sha256_hex, LMSTUDIO_URL, LMSTUDIO_MODEL, 
    MEILI_URL, MEILI_API_KEY, MEILI_INDEX, QDRANT_URL, QDRANT_COLLECTION, 
    EMB_MODEL, DEFAULT_RESIZE, DEFAULT_QUALITY, DEFAULT_TIMEOUT, PROMPT
)
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from audio_analyzer_v2 import analyze_audio_comprehensive

# å‹•ç”»å¯¾å¿œæ‹¡å¼µå­
VIDEO_EXTENSIONS = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm', '.m4v', '.mpg', '.mpeg'}

def check_ffmpeg():
    """FFmpegãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    try:
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  FFmpegãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False

def safe_parse_framerate(framerate_str: str) -> float:
    """ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆæ–‡å­—åˆ—ã‚’å®‰å…¨ã«è§£æ"""
    if '/' in framerate_str:
        try:
            num, denom = framerate_str.split('/', 1)
            return float(num) / float(denom)
        except (ValueError, ZeroDivisionError):
            return 0.0
    try:
        return float(framerate_str)
    except ValueError:
        return 0.0

def get_video_info(video_path: Path) -> Dict:
    """å‹•ç”»ã®æƒ…å ±ã‚’å–å¾—"""
    try:
        probe = ffmpeg.probe(str(video_path))
        video_stream = next((s for s in probe['streams'] if s['codec_type'] == 'video'), None)
        
        if video_stream:
            duration = float(probe['format']['duration'])
            width = int(video_stream['width'])
            height = int(video_stream['height'])
            fps = safe_parse_framerate(video_stream['r_frame_rate'])  # å®‰å…¨ãªè§£ææ–¹æ³•
            
            return {
                "duration": duration,
                "width": width,
                "height": height,
                "fps": fps,
                "codec": video_stream.get('codec_name', 'unknown')
            }
    except Exception as e:
        print(f"å‹•ç”»æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return {"duration": 0, "width": 0, "height": 0, "fps": 0, "codec": "unknown"}

def has_audio_stream(video_path: Path) -> bool:
    try:
        probe = ffmpeg.probe(str(video_path))
        return any(s for s in probe.get('streams', []) if s.get('codec_type') == 'audio')
    except Exception:
        return False

def extract_audio_track(video_path: Path) -> Optional[Path]:
    """å‹•ç”»ã‹ã‚‰éŸ³å£°ãƒˆãƒ©ãƒƒã‚¯ã‚’æŠ½å‡ºï¼ˆãƒ¢ãƒãƒ©ãƒ«16kHz WAVï¼‰ã€‚ãªã‘ã‚Œã°None"""
    if not has_audio_stream(video_path):
        return None
    temp_dir = Path(tempfile.mkdtemp())
    out_wav = temp_dir / "audio_track.wav"
    try:
        (
            ffmpeg
            .input(str(video_path))
            .output(str(out_wav), acodec='pcm_s16le', ac=1, ar=16000, vn=None)
            .overwrite_output()
            .run(quiet=True)
        )
        if out_wav.exists() and out_wav.stat().st_size > 0:
            return out_wav
    except Exception as e:
        print(f"éŸ³å£°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
    # å¤±æ•—æ™‚ã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    try:
        shutil.rmtree(temp_dir, ignore_errors=True)
    except Exception:
        pass
    return None

def extract_frames_ffmpeg(video_path: Path, interval_seconds: float = 5.0, max_frames: int = 10) -> List[Path]:
    """FFmpegã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã‹ã‚‰å®šæœŸçš„ã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡º"""
    temp_dir = Path(tempfile.mkdtemp())
    frames = []
    
    try:
        info = get_video_info(video_path)
        duration = info['duration']
        
        if duration <= 0:
            print(f"âš ï¸  å‹•ç”»ã®é•·ã•ã‚’å–å¾—ã§ãã¾ã›ã‚“: {video_path.name}")
            return []
        
        # æŠ½å‡ºã™ã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—
        timestamps = []
        current_time = 1.0  # 1ç§’ã‹ã‚‰é–‹å§‹
        while current_time < duration and len(timestamps) < max_frames:
            timestamps.append(current_time)
            current_time += interval_seconds
        
        print(f"ğŸ“¹ {len(timestamps)}æšã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºä¸­...")
        
        # FFmpegã§ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        for i, ts in enumerate(tqdm(timestamps, desc="ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º")):
            output_path = temp_dir / f"frame_{i:04d}.jpg"
            
            try:
                (
                    ffmpeg
                    .input(str(video_path), ss=ts)
                    .output(str(output_path), vframes=1, format='image2', vcodec='mjpeg')
                    .overwrite_output()
                    .run(quiet=True)
                )
                frames.append(output_path)
            except Exception as e:
                print(f"ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼ (æ™‚åˆ» {ts:.1f}ç§’): {e}")
                continue
        
        return frames
        
    except Exception as e:
        print(f"ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        shutil.rmtree(temp_dir, ignore_errors=True)
        return []

def process_video_with_existing_system(video_path: Path, args):
    """æ—¢å­˜ã®process_imageé–¢æ•°ã¨åŒã˜æµã‚Œã§å‹•ç”»ã‚’å‡¦ç†"""
    
    # å‹•ç”»ã®ãƒ‘ã‚¹æƒ…å ±ã‚’æº–å‚™ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜ï¼‰
    rel = str(video_path.resolve())
    path_hash = path_sha256_hex(rel)
    media_id = int(path_hash[:16], 16)
    
    print(f"[DEBUG] Processing video: {rel}")
    print(f"[DEBUG] Path hash: {path_hash}")
    print(f"[DEBUG] Using converted ID: {media_id}")
    
    try:
        # 1. ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
        frames = extract_frames_ffmpeg(video_path, args.interval, args.max_frames)
        if not frames:
            print(f"âš ï¸  ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå¤±æ•—: {video_path.name}")
            return False
        
        # 2. å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è§£æï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜æ–¹æ³•ï¼‰
        all_captions = []
        all_tags = set()
        
        print(f"ğŸ” {len(frames)}æšã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è§£æä¸­...")
        for i, frame_path in enumerate(tqdm(frames, desc="ãƒ•ãƒ¬ãƒ¼ãƒ è§£æ")):
            try:
                # æ—¢å­˜ã®call_lmstudio_vlmé–¢æ•°ã‚’ä½¿ç”¨
                res = call_lmstudio_vlm(
                    args.lmstudio_url, args.model, PROMPT, frame_path,
                    resize_width=args.resize_width, quality=args.quality, timeout=args.timeout
                )
                
                if res:
                    parsed = res.get("parsed", {"caption": "", "tags": []})
                    if parsed.get("caption"):
                        all_captions.append(f"ãƒ•ãƒ¬ãƒ¼ãƒ {i+1}: {parsed['caption']}")
                    if parsed.get("tags"):
                        all_tags.update(parsed["tags"])
                        
            except Exception as e:
                print(f"ãƒ•ãƒ¬ãƒ¼ãƒ {i+1}ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # 3. å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        video_info = get_video_info(video_path)
        duration_str = f"{video_info['duration']:.1f}ç§’" if video_info['duration'] > 0 else "ä¸æ˜"
        resolution_str = f"{video_info['width']}x{video_info['height']}" if video_info['width'] > 0 else "ä¸æ˜"

        # 3.5 éŸ³å£°è§£æï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        audio_summary = None
        audio_tags: List[str] = []
        audio_payload: Dict = { "has_audio": False }
        transcript_text_value = ""
        audio_path = extract_audio_track(video_path)
        if audio_path:
            try:
                # Whisperã®æ—¢å®šãƒ¢ãƒ‡ãƒ«ã‚’ 'small' ã«å¼•ãä¸Šã’ï¼ˆæ­Œå”±ã®æ—¥æœ¬èªèªè­˜ç²¾åº¦ã‚’æ”¹å–„ï¼‰
                whisper_model = os.getenv('WHISPER_MODEL', 'small')
                audio_result = analyze_audio_comprehensive(audio_path, use_whisper=True, whisper_model=whisper_model)
                # åˆ¤å®š: ç„¡éŸ³ã‹ã©ã†ã‹ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
                feats = audio_result.get('features', {})
                rms_mean = float(feats.get('rms_mean') or 0.0)
                transcript_text = ((audio_result.get('transcription') or {}).get('text') or '').strip()
                transcript_text_value = transcript_text
                audio_type = feats.get('audio_type') or ('silent' if (rms_mean < 0.005 and not transcript_text) else 'ambiguous')
                audio_summary = audio_result.get('description')
                audio_tags = list(set(audio_result.get('tags') or []))
                audio_payload = {
                    "has_audio": True,
                    "audio_type": audio_type,
                    "caption": audio_summary,
                    "tags": audio_tags,
                    "has_transcription": bool(transcript_text),
                    "language": (audio_result.get('transcription') or {}).get('language'),
                    "speaker_gender": audio_result.get('speaker_gender')
                }
            except Exception as e:
                print(f"éŸ³å£°è§£æã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                # ä¸€æ™‚WAVã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                try:
                    if audio_path and audio_path.exists():
                        shutil.rmtree(audio_path.parent, ignore_errors=True)
                except Exception:
                    pass

        # 4. çµæœã‚’çµ±åˆï¼ˆéŸ³å£°è¦ç´„ã‚‚ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã«åæ˜ ï¼‰
        base_caption = f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ« ({duration_str})"
        frames_caption = " / ".join(all_captions[:3]) if all_captions else ""
        audio_caption_part = ""
        if audio_payload.get('has_audio'):
            # æ­Œè©æŠœç²‹ã¯å¸¸ã«å„ªå…ˆã—ã¦è¡¨ç¤ºï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            parts = []
            if transcript_text_value:
                snippet = transcript_text_value[:120]
                parts.append(f"æ­Œè©æŠœç²‹: ã€{snippet}ã€")
            if audio_summary:
                # æ­Œè©æŠœç²‹ãŒã‚ã‚‹å ´åˆã¯è¦ç´„ã‚’å°‘ã—çŸ­ã‚ã«
                limit = 120 if transcript_text_value else 140
                short_audio = audio_summary if len(audio_summary) <= limit else audio_summary[:limit] + 'â€¦'
                # è©±è€…æ€§åˆ¥è¡¨è¨˜
                gender = audio_payload.get('speaker_gender') or ''
                gender_text = "ï¼ˆè©±è€…: ç”·æ€§ï¼‰" if gender == 'male' else ("ï¼ˆè©±è€…: å¥³æ€§ï¼‰" if gender == 'female' else '')
                parts.append(f"éŸ³å£°è¦ç´„: {short_audio}{gender_text}")
            audio_caption_part = " / ".join(parts)
        parts = [p for p in [base_caption, frames_caption, audio_caption_part] if p]
        combined_caption = " : ".join(parts)

        combined_tags = list(all_tags) + ["å‹•ç”»", "video", duration_str, resolution_str, video_info['codec']]
        if audio_payload.get('has_audio'):
            atype = audio_payload.get('audio_type')
            if atype == 'speech':
                combined_tags += ["éŸ³å£°ã‚ã‚Š", "ä¼šè©±"]
            elif atype == 'music':
                combined_tags += ["éŸ³å£°ã‚ã‚Š", "æ¥½æ›²", "music"]
                if audio_payload.get('has_transcription'):
                    combined_tags += ["æ­Œè©ã‚ã‚Š", "ãƒœãƒ¼ã‚«ãƒ«"]
            elif atype == 'sfx':
                combined_tags += ["éŸ³å£°ã‚ã‚Š", "åŠ¹æœéŸ³"]
            elif atype == 'silent':
                combined_tags += ["ç„¡éŸ³"]
            # è©±è€…æ€§åˆ¥ã‚¿ã‚°
            gender = audio_payload.get('speaker_gender')
            if gender == 'male':
                combined_tags += ["ç”·æ€§å£°"]
            elif gender == 'female':
                combined_tags += ["å¥³æ€§å£°"]
            # è§£æã‚¿ã‚°ã‚‚ä¸€éƒ¨è¶³ã™ï¼ˆå¤šã™ããªã„ã‚ˆã†ã«ï¼‰
            combined_tags += audio_tags[:5]

        # 5. çµæœã‚’JSONã¨ã—ã¦ä½œæˆï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ äº’æ›ï¼‰
        result_json = {
            "caption": combined_caption,
            "tags": combined_tags,
            "video_metadata": video_info,
            "analyzed_frames": len(frames),
            "audio": audio_payload
        }
        raw_json = json.dumps(result_json, ensure_ascii=False)
        
        # 6. Meilisearchã«ä¿å­˜ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜ï¼‰
        print(f"[DEBUG] Starting Meilisearch indexing for {rel}")
        try:
            upsert_meilisearch(
                MEILI_URL, MEILI_API_KEY, MEILI_INDEX, 
                media_id, rel, path_hash, 
                combined_caption, combined_tags, args.model, raw_json
            )
            print(f"[DEBUG] Meilisearch indexing completed for {rel}")
        except Exception as e:
            print(f"[WARNING] Meilisearch indexing failed for {rel}: {e}")
        
        # 7. Qdrantã«ä¿å­˜ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨å®Œå…¨ã«åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        print(f"[DEBUG] Starting Qdrant embedding for {rel}")
        try:
            qdrant_client = QdrantClient(url=QDRANT_URL)
            
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜ï¼‰
            emb_model_name = os.getenv("EMB_MODEL", "all-MiniLM-L6-v2")
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆéŸ³å£°èª¬æ˜/ã‚¿ã‚°ã‚‚å«ã‚ã‚‹ï¼‰
            audio_text = (audio_summary or '') + ' ' + ' '.join(audio_tags)
            search_text = f"{combined_caption} {' '.join(combined_tags)} {audio_text}"
            print(f"[DEBUG] Embedding text: {search_text}")
            
            if "plamo-embedding" in emb_model_name.lower():
                # Plamo embeddingã®å ´åˆï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜å‡¦ç†ï¼‰
                import torch
                from transformers import AutoTokenizer, AutoModel
                
                # ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆ
                trusted_models = [
                    "pfnet/plamo-embedding-1b",
                    "sentence-transformers/all-MiniLM-L6-v2",
                    "sentence-transformers/all-mpnet-base-v2"
                ]
                
                trust_code = emb_model_name in trusted_models
                if not trust_code:
                    print(f"[SECURITY WARNING] Untrusted model: {emb_model_name}. Using trust_remote_code=False")
                
                tokenizer = AutoTokenizer.from_pretrained(emb_model_name, trust_remote_code=trust_code)
                plamo_model = AutoModel.from_pretrained(emb_model_name, trust_remote_code=trust_code)
                
                device = "cuda" if torch.cuda.is_available() else "cpu"
                plamo_model = plamo_model.to(device)
                
                with torch.inference_mode():
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«ã¯encode_documentãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
                    embedding_tensor = plamo_model.encode_document([search_text], tokenizer)
                    embedding = embedding_tensor[0].cpu().numpy()
                    print(f"[DEBUG] Plamo embedding generated, size: {len(embedding)}")
                    
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºä¿ï¼ˆPlamo embeddingã®ã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ï¼‰
                ensure_qdrant_collection(qdrant_client, args.qdrant_collection, len(embedding))
            else:
                # é€šå¸¸ã®sentence-transformersï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜å‡¦ç†ï¼‰
                sb_model = SentenceTransformer(emb_model_name)
                # Cosineå®‰å®šåŒ–ã®ãŸã‚æ­£è¦åŒ–
                emb = sb_model.encode([search_text], convert_to_numpy=True, normalize_embeddings=True)[0]
                embedding = emb / (np.linalg.norm(emb) or 1.0)
                print(f"[DEBUG] Sentence-transformers embedding generated, size: {len(embedding)}")
                
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºä¿
                ensure_qdrant_collection(qdrant_client, args.qdrant_collection, len(embedding))
            
            # Qdrantã«ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜
            upsert_qdrant_vector(
                qdrant_client, args.qdrant_collection, media_id, rel, 
                combined_caption, combined_tags, args.model, embedding.tolist()
            )
            print(f"[DEBUG] Qdrant embedding completed for {rel}")
            
        except Exception as e:
            print(f"[WARNING] Qdrant embedding failed for {rel}: {e}")
        
        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        print(f"âœ… ç™»éŒ²å®Œäº†: {video_path.name}")
        print(f"   ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³: {combined_caption[:100]}...")
        print(f"   ã‚¿ã‚°: {', '.join(combined_tags[:10])}")
        
        # server.jsç”¨ã®æ§‹é€ åŒ–ã•ã‚ŒãŸJSONçµæœã‚’å‡ºåŠ›
        video_result = {
            "success": True,
            "file_name": video_path.name,
            "caption": combined_caption,
            "tags": combined_tags,
            "video_metadata": video_info,
            "analyzed_frames": len(frames),
            "media_id": media_id,
            "model": args.model
        }
        print(f"VIDEO_ANALYSIS_RESULT: {json.dumps(video_result, ensure_ascii=False)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {video_path.name}")
        print(f"   è©³ç´°: {str(e)}")
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚JSONã§å‡ºåŠ›
        error_result = {
            "success": False,
            "file_name": video_path.name,
            "error": str(e)
        }
        print(f"VIDEO_ANALYSIS_RESULT: {json.dumps(error_result, ensure_ascii=False)}")
        
        return False
        
    finally:
        # ä¸€æ™‚ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if 'frames' in locals() and frames:
            temp_dir = frames[0].parent
            shutil.rmtree(temp_dir, ignore_errors=True)

def main():
    parser = argparse.ArgumentParser(description="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£æï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ æº–æ‹ ï¼‰")
    parser.add_argument("--video", type=str, help="å˜ä¸€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--video_dir", type=str, help="å‹•ç”»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹")
    parser.add_argument("--lmstudio_url", type=str, default=LMSTUDIO_URL)
    parser.add_argument("--model", type=str, default=LMSTUDIO_MODEL)
    parser.add_argument("--resize_width", type=int, default=DEFAULT_RESIZE)
    parser.add_argument("--quality", type=int, default=DEFAULT_QUALITY)
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    parser.add_argument("--interval", type=float, default=5.0, help="ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–“éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--max_frames", type=int, default=10, help="æœ€å¤§æŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ æ•°")
    parser.add_argument("--emb_model", type=str, default=EMB_MODEL)
    parser.add_argument("--qdrant_collection", type=str, default=QDRANT_COLLECTION)
    
    args = parser.parse_args()
    
    if not args.video and not args.video_dir:
        parser.print_help()
        sys.exit(1)
    
    # FFmpegãƒã‚§ãƒƒã‚¯
    if not check_ffmpeg():
        sys.exit(1)
    
    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
    load_env()
    
    # å‡¦ç†å¯¾è±¡ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
    video_files = []
    
    if args.video:
        video_path = Path(args.video)
        if video_path.exists() and video_path.suffix.lower() in VIDEO_EXTENSIONS:
            video_files.append(video_path)
        else:
            print(f"âš ï¸  å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.video}")
    
    if args.video_dir:
        video_dir = Path(args.video_dir)
        if video_dir.exists():
            for ext in VIDEO_EXTENSIONS:
                video_files.extend(video_dir.rglob(f"*{ext}"))
                video_files.extend(video_dir.rglob(f"*{ext.upper()}"))
        else:
            print(f"âš ï¸  ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.video_dir}")
    
    video_files = list(set(video_files))
    print(f"ğŸ“Š {len(video_files)}å€‹ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™")
    
    success_count = 0
    # å„å‹•ç”»ã‚’å‡¦ç†
    for i, video_path in enumerate(video_files, 1):
        print(f"\n[{i}/{len(video_files)}]")
        if process_video_with_existing_system(video_path, args):
            success_count += 1
    
    print(f"\nâœ… å‡¦ç†å®Œäº†: {success_count}/{len(video_files)}å€‹ã®å‹•ç”»ã‚’æ­£å¸¸ã«å‡¦ç†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()
